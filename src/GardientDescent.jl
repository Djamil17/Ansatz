"""

""" 

using ForwardDiff

mag(ùêØ::Vector)=sqrt(sum(ùêØ.^2))

##TODO : multidimensional gradient descent (done)
##TODO: stochastic gradient descent 
##TODO: adaptive gradient descents  
##TODO: training set GD 

function gradient_descent(objective::Function, Œ∏::Float64, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Float64

    """

    gradient_descent(objective::Function, Œ∏::Float64, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Float64

    Performs gradient descent in n-d , for n variables, minimizes objective f with n arguments, given a guess vector Œ∏ and a learning rate Œ∑.
    Exits once the absolute value of the step Œ∑*||‚àá(f)|| < tolerance. 

    Examples
    ‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°

    julia> f(x)=x^2+2.324245x
    julia> gradient_descent(f,-1,.00001,1000000,1e-6)
    
    -1.1621220000061123


    """

    i=0
    x·µ¢=Œ∏
    while i<iterations
        loss=‚àá(objective,x·µ¢)
        x·µ¢‚Çä‚ÇÅ= x·µ¢-Œ∑*loss
        if abs(loss)<tolerance
            break
        end 
        x·µ¢=x·µ¢‚Çä‚ÇÅ
        i+=1
    end 

    return x·µ¢

end 


function gradient_descent(objective::Function, Œ∏::Array{Float64,1}, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Vector

    """

    gradient_descent(objective::Function, Œ∏::Float64, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Float64

    Performs gradient descent in n-d , for n variables, minimizes objective f with n arguments, given a guess vector Œ∏ and a learning rate Œ∑.
    Exits once the absolute value of the step Œ∑*||‚àá(f)|| < tolerance. 

    Examples
    ‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°

    julia> f(x::Vector)=x[1]^2+2.324245x[2]
    julia> gradient_descent(f,[-1.0,-2.0],.00001,1000000,1e-6)
    
    2-element Array{Float64,1}:
    -2.0607414274390493e-9
    -25.242449999476282
        

    """
    i=0
    w·µ¢=Œ∏
    
    while i<iterations
        loss=‚àá·µ£(objective,w·µ¢)
        w·µ¢‚Çä‚ÇÅ= w·µ¢-Œ∑*loss
        if mag(loss)<tolerance
            break
        end 
        w·µ¢=w·µ¢‚Çä‚ÇÅ
        i+=1
    end 

    return w·µ¢

end 

function stochastic_gradient_descent(objective::Function, Œ∏::Array{Float64,1}, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Vector

    i=0
    w·µ¢=Œ∏
    
    while i<iterations
        sample=rand(w·µ¢)
        loss=‚àá(objective,sample)
        w·µ¢‚Çä‚ÇÅ= w·µ¢-Œ∑* loss
        if mag(loss)<tolerance
            break
        end 
        w·µ¢=w·µ¢‚Çä‚ÇÅ
        i+=1
    end 

    return w·µ¢

end 

function minibatch_sgd()

end 

function Œîœµ(œà::Function, HÃÇ::Function, Œµ‚ÇÄ::Function, N::Integer,ùê±::Vector, A·µ¢) 
    ùí™=zeros(3) ## expected values 
    for i in 1:N
        log‚àá=‚àá(log_œà,[ùê±[i], A·µ¢])[end]
        ùí™[1]+=1/œà([ùê±[i], A·µ¢])*log‚àá*HÃÇ([ùê±[i], A·µ¢],œà)
        ùí™[2]+=Œµ‚ÇÄ([ùê±[i], A·µ¢],œà)
        ùí™[3]+=log‚àá
    end 
    return 2*ùí™[1]/N-2*ùí™[2]/N*ùí™[3]/N
end 

function vmc(N,Œ∏,Œ∑,iterations,tolerance,œà¬≤,HÃÇ,Œµ‚ÇÄ)
    i=0
    A·µ¢=Œ∏
    while i<iterations
        X=metropolis_hastings(œµ,xmin,xmax,N,A·µ¢,œà¬≤)
        loss=Œîœµ(œà,HÃÇ,Œµ‚ÇÄ, N,X,A·µ¢)
        # @show loss
        A·µ¢‚Çä‚ÇÅ=A·µ¢-Œ∑*loss
        if abs(loss)<tolerance
            break
        end 
        A·µ¢=A·µ¢‚Çä‚ÇÅ
        i+=1
    end 
    return A·µ¢
end 





