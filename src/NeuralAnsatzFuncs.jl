"""
Script: NeuralNetAnsatz
Author:Djamil Lakhdar-Hamina
Date: 06/28/2023
Description:

A script to test quantum monte carlo for calculating ground state 

"""

include("NeuralAnsatz.jl")
include("MetropolisHastings.jl")
include("Jackknife.jl")
include("VectorDifferentials.jl")

const Ïµ=.5f0
const xmin1=-1.0f0
const xmax1=1.0f0
const Î»=.01

struct NeuralAnsatz
    chain::Chain
end

Î©=1f-4
Î±=-0.5f0 

function (m::NeuralAnsatz)(x)
   
    return exp.(Î©*m.chain(sort(x)).-Î±*(x[1].^2 .+x[2].^2))
end


# function (m::NeuralAnsatz)(x)
   
#     return exp.(m.chain(x).+Î±*(x.^2))
# end


Flux.@functor NeuralAnsatz

âˆ‡(g::T,ğ±::Vector) where {T<:Union{NeuralAnsatz,Function}}=gradient(ğ±->sum(g(ğ±)),ğ±)
âˆ‡Â²(g::T,ğ±::Vector) where {T<:Union{NeuralAnsatz,Function}}=sum(Diagonal(hessian(ğ±->sum(g(ğ±)),ğ±)))

Ï•(ğ±::Vector{T}) where {T<:AbstractFloat}=1/2*ğ±[1]^2+Î»*ğ±[1]^4
HÌ‚(ğ±::Vector{T1}, Ïˆ::T2) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function}}=-âˆ‡Â²(Ïˆ,ğ±)/2 .+Ï•(ğ±)*Ïˆ(ğ±)
Îµâ‚€(ğ±::T1,Ïˆ::T2) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function}}=Ïˆ(ğ±).^-1 .*HÌ‚(ğ±,Ïˆ)
Îµâ‚€(ğ±::Vector{T1},Ïˆ::T2) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function}}=Ïˆ(ğ±).^-1 .*HÌ‚(ğ±,Ïˆ)

## default one layer NN 
function buildChain(input::T1, width::T1,output::T1,Ïƒ::T2) where {T1<:Integer,T2<:Function}
    return Chain(Dense(input, width,Ïƒ),Dense(width, output,Ïƒ))
end 

## TODO : make recursiev for neatness 
function buildChain(input::T, width::T,depth::T, output::T,Ïƒ::T2) where {T<:Integer,T2<:Function}
    
    ## need to build a string
    header="Chain("
    layers="Dense($input,$width,$Ïƒ)"
    closing="Dense($width,$output,$Ïƒ))"

    if depth==0
       chainString=header*layers*")"
    else
        chainString=layers
        for layer in 1:depth
            if layer==depth
                chainString=header*chainString*","*closing
            else 
                chainString=chainString*","*"Dense($width,$width,$Ïƒ)"
            end 
        end 
    end 

    chainExpr=Base.Meta.parse(chainString)        
    return eval(chainExpr)
end 

function Î”Ïµ(N::Integer,ğ«::Vector{T1},Î¨::T2,HÌ‚::T3,Îµâ‚€::T4,Î˜::Params{Zygote.Buffer{Any, Vector{Any}}}) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function},T3<:Function,T4<:Function}
    logâˆ‡0=gradient(()->sum(log.(Î¨([ğ«[1]]))),Î˜)
    ğ’ª=[1/Î¨([ğ«[1]])*HÌ‚([ğ«[1]],Î¨).*logâˆ‡0,Îµâ‚€([ğ«[1]],Î¨),logâˆ‡0]
    @simd for i in 2:N
        logâˆ‡=gradient(()->sum(log.(Î¨([ğ«[i]]))),Î˜)
        ğ’ª[1].+=1/Î¨([ğ«[i]])*HÌ‚([ğ«[i]],Î¨).*logâˆ‡0 ## return grads objects 
        ğ’ª[2].+=Îµâ‚€([ğ«[i]],Î¨)
        ğ’ª[3].+=logâˆ‡
    end 
    return 2.0f32 .*ğ’ª[1]./N  .- 2.0f32 .* ((ğ’ª[2]./N)[1] .* ğ’ª[3]./N)
end 

function Î”Ïµ(N::Integer,ğ«::Vector{Vector{T1}},Î¨::T2,HÌ‚::T3,Îµâ‚€::T4,Î˜::Params{Zygote.Buffer{Any, Vector{Any}}}) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function},T3<:Function, T4<:Function}
    logâˆ‡0=gradient(()->sum(log.(Î¨(ğ«[1]))),Î˜)
    ğ’ª=[1/Î¨(ğ«[1])*HÌ‚(ğ«[1],Î¨).*logâˆ‡0,Îµâ‚€(ğ«[1],Î¨),logâˆ‡0]
    @simd for i in 2:N
        logâˆ‡=gradient(()->sum(log.(Î¨(ğ«[i]))),Î˜)
        ğ’ª[1].+=1/Î¨(ğ«[i])*HÌ‚(ğ«[i],Î¨).*logâˆ‡0 ## return grads objects 
        ğ’ª[2].+=Îµâ‚€(ğ«[i],Î¨)
        ğ’ª[3].+=logâˆ‡
    end 
    return 2.0f32 .*ğ’ª[1]./N   .- 2.0f32 .* ((ğ’ª[2]./N)[1] .* ğ’ª[3]./N)
end 


## 1 particle 
function EÌ„(ğ±::Vector{T1}, Ïˆ::T2) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function}}
    N=length(ğ±)
    sum_=0
    for i in 1:N
        sum_+=Îµâ‚€([ğ±[i]],Ïˆ)[1]
    end 
    1/N*sum_
end 

## n particle 
function EÌ„(ğ±::Vector{Vector{T1}}, Ïˆ::NeuralAnsatz) where {T1<:AbstractFloat}
    N=length(ğ±)
    sum_=0
    for i in 1:N
        sum_+=Îµâ‚€(ğ±[i],Ïˆ)[1]
    end 
    1/N*sum_
end 

function train!(epoch::T1,Ïµ::T2,xmin1::T3,xmax1::T3,N::T1,Î¨::T4,HÌ‚::T5,Îµâ‚€::T6,Î˜::T7,opt::T8) where {T1<:Integer,T2<:Real, T3<:Real, T4<:Union{NeuralAnsatz,Function}, T5<:Function,T6<:Function,T7<:Params{Zygote.Buffer{Any, Vector{Any}}},T8<:Flux.Optimise.AbstractOptimiser}
    println("training with $N sample points and $epoch iterations...")
    for i in 1:epoch
        ğ«=metropolis_hastings(Float32,Ïµ,xmin1,xmax1,N,x->sum(Î¨([x]).^2))
        Î”E=Î”Ïµ(N,ğ«,Î¨,HÌ‚,Îµâ‚€,Î˜) ## custom gradient 
        # @show "iteration $i: $(g.params)"
        Flux.update!(opt,Î˜,Î”E)
    end 
    println("trained!")
    # println("Params: $Î˜")
end 

function train!(particles::T0,epoch::T1,Ïµ::T2,xmin1::T3,xmax1::T3,N::T1,Î¨::T4,HÌ‚::T5,Îµâ‚€::T6,Î˜::T7,opt::T8) where {T0<:Integer,T1<:Integer,T2<:Real, T3<:Real, T4<:Union{NeuralAnsatz,Function}, T5<:Function,T6<:Function,T7<:Params{Zygote.Buffer{Any, Vector{Any}}},T8<:Flux.Optimise.AbstractOptimiser}
    println("training with $N sample points and $epoch iterations...")
    for i in 1:epoch
        ğ«=metropolis_hastings(particles,Ïµ,xmin1,xmax1,N,x->sum(Î¨(x).^2))
        Î”E=Î”Ïµ(N,ğ«,Î¨,HÌ‚,Îµâ‚€,Î˜) ## custom gradient 
        Flux.update!(opt,Î˜,Î”E)
    end 
    println("trained!")
end 

"""
Start of program...
"""

opt=Adam(.1)
depth=3
width=100
N=1000
epochs=100
trials=[1,5,10,25,50,100]
depths=[1,2,3]

function test_width()

    variational_info=[]
    for width in trials
        println("Calculating $width ...")
        chain=buildChain(1,width,depth,1,relu)
        Î¨=NeuralAnsatz(chain)
        Î˜=Flux.params(Î¨)
        @time train!(epochs,Ïµ,xmin1,xmax1,N, Î¨,HÌ‚,Îµâ‚€,Î˜,opt)
        @time E_jack,Î”E=jackknife(metropolis_hastings(Float32, Ïµ,xmin1,xmax1,1000,x->sum(Î¨([x]).^2)),Îµâ‚€,Î¨)
        push!(variational_info,(width,E_jack,Î”E))
        println("Calculation finished!")

    end 
    return variational_info
end

function test_depth()
    width=10
    variational_info=[]
    for depth in depths
        println("Calculating $depth ...")
        chain=buildChain(1,width,depth,1,relu)
        Î¨=NeuralAnsatz(chain)
        Î˜=Flux.params(Î¨)
        @time train!(epochs,Ïµ,xmin1,xmax1,N, Î¨,HÌ‚,Îµâ‚€,Î˜,opt)
        @time E_jack,Î”E=jackknife(metropolis_hastings(Float32, Ïµ,xmin1,xmax1,1000,x->sum(Î¨([x]).^2)),Îµâ‚€,Î¨)
        push!(variational_info,(depth,E_jack,Î”E))
        println("Calculation finished!")

    end 
    return variational_info
end

function main()
    return test_depth(),test_width()
end 

variational_info=main()
depth_E,width_E=variational_info[1],variational_info[2]
E1,uncertainty1=[t[2] for t in depth_E],[t[3] for t in depth_E]
E2,uncertainty2=[t[2] for t in width_E],[t[3] for t in width_E]




