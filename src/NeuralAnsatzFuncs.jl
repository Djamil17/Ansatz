"""
Script: NeuralNetAnsatz
Author:Djamil Lakhdar-Hamina
Date: 06/28/2023
Description:

A script to test quantum monte carlo for calculating ground state 

"""

include("MetropolisHastings.jl")
include("Jackknife.jl")
include("VectorDifferentials.jl")

## TODO : make recursiev for neatness 
function buildChain(;input::T1, width::T2,output::T3=1,depth::T4=1,Ïƒ::T5=celu) where {T1<:Integer,T2<:Integer,T3<:Integer,T4<:Integer,T5<:Function}
    
    ## need to build a string
    header="Chain("
    layers="Dense($input,$width,$Ïƒ)"
    closing="Dense($width,$output,$Ïƒ))"

    if depth==0
       chainString=header*layers*")"
    else
        chainString=layers
        for layer in 1:depth
            if layer==depth
                chainString=header*chainString*","*closing
            else 
                chainString=chainString*","*"Dense($width,$width,$Ïƒ)"
            end 
        end 
    end 

    chainExpr=Base.Meta.parse(chainString)        
    return eval(chainExpr)
end 

# function Î”Ïµ(N::Integer,ğ«::Vector{T1},Î¨::T2,HÌ‚::T3,Îµâ‚€::T4,Î˜::Params{Zygote.Buffer{Any, Vector{Any}}}) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function},T3<:Function,T4<:Function}
#     logâˆ‡0=gradient(()->sum(log.(Î¨([ğ«[1]]))),Î˜)
#     ğ’ª=[1/Î¨([ğ«[1]])*HÌ‚([ğ«[1]],Î¨).*logâˆ‡0,Îµâ‚€([ğ«[1]],Î¨),logâˆ‡0]
#     @inbounds @fastmath @simd for i in 2:N
#         logâˆ‡=gradient(()->sum(log.(Î¨([ğ«[i]]))),Î˜)
#         ğ’ª[1].+=1/Î¨([ğ«[i]])*HÌ‚([ğ«[i]],Î¨).*logâˆ‡0 ## return grads objects 
#         ğ’ª[2].+=Îµâ‚€([ğ«[i]],Î¨)
#         ğ’ª[3].+=logâˆ‡
#     end 
#     return 2.0f32 .*ğ’ª[1]./N  .- 2.0f32 .* ((ğ’ª[2]./N)[1] .* ğ’ª[3]./N)
# end 

function Î”Ïµ(N::Integer,ğ«::Vector{Vector{T1}},Î¨::T2,HÌ‚::T3,Îµâ‚€::T4,Î˜::Params{Zygote.Buffer{Any, Vector{Any}}}) where {T1<:AbstractFloat,T2<:Union{NeuralAnsatz,Function},T3<:Function, T4<:Function}
    logâˆ‡0=gradient(()->sum(log.(Î¨(ğ«[1]))),Î˜)
    ğ’ª=[1/Î¨(ğ«[1])*HÌ‚(ğ«[1],Î¨).*logâˆ‡0,Îµâ‚€(ğ«[1],Î¨),logâˆ‡0]
    @inbounds @fastmath @simd for i in 2:N
        logâˆ‡=gradient(()->sum(log.(Î¨(ğ«[i]))),Î˜)
        ğ’ª[1].+=1/Î¨(ğ«[i])*HÌ‚(ğ«[i],Î¨).*logâˆ‡0 ## return grads objects 
        ğ’ª[2].+=Îµâ‚€(ğ«[i],Î¨)
        ğ’ª[3].+=logâˆ‡
    end 
    return 2.0f00 .*ğ’ª[1]./N   .- 2.0f00 .* ((ğ’ª[2]./N)[1] .* ğ’ª[3]./N)
end 

## n particle 
function EÌ„(ğ±::Vector{Vector{T1}}, Îµâ‚€::T2,Ïˆ::T3) where {T1<:AbstractFloat,T2<:Function,T3<:Union{NeuralAnsatz,Function}}
    N=length(ğ±)
    sum_=0
    @inbounds @fastmath @simd for i in 1:N
        sum_+=Îµâ‚€(ğ±[i],Ïˆ)[1]
    end 
    1/length(ğ±)*sum_
end 

function train!(epoch::T1,Ïµ::T2,xmin1::T3,xmax1::T3,N::T1,Î¨::T4,HÌ‚::T5,Îµâ‚€::T6,Î˜::T7,opt::T8) where {T1<:Integer,T2<:Real, T3<:Real, T4<:Union{NeuralAnsatz,Function}, T5<:Function,T6<:Function,T7<:Params{Zygote.Buffer{Any, Vector{Any}}},T8<:Flux.Optimise.AbstractOptimiser}
    println("training with $N sample points and $epoch iterations...")
    for _ in 1:epoch
        ğ«=metropolis_hastings(Float32,Ïµ,xmin1,xmax1,N,x->sum(Î¨([x]).^2))
        Î”E=Î”Ïµ(N,ğ«,Î¨,HÌ‚,Îµâ‚€,Î˜) ## custom gradient 
        Flux.update!(opt,Î˜,Î”E)
    end 
    println("trained!")
end 

function train!(particles::T0,epoch::T1,Ïµ::T2,xmin::T3,xmax::T3,N::T1,Î¨::T4,HÌ‚::T5,Îµâ‚€::T6,Î˜::T7,opt::T8) where {T0<:Integer,T1<:Integer,T2<:Real, T3<:Real, T4<:Union{NeuralAnsatz,Function}, T5<:Function,T6<:Function,T7<:Params{Zygote.Buffer{Any, Vector{Any}}},T8<:Flux.Optimise.AbstractOptimiser}
    println("training with $N sample points and $epoch iterations...")
    for _ in 1:epoch
        ğ«=metropolis_hastings(
            particles,
            Ïµ,
            xmin,xmax,
            N,x->sum(Î¨(x).^2),
            distribution=Uniform
            )

        Î”E=Î”Ïµ(N,ğ«,Î¨,HÌ‚,Îµâ‚€,Î˜) ## custom gradient 
        Flux.update!(opt,Î˜,Î”E)
    end 
    println("trained!")
end 

function train!(vmc_problem::vmcProblem,vmc_solution::vmcSolution,opt::Flux.Optimise.AbstractOptimiser,HÌ‚::T1,Îµâ‚€::T2,Î˜::T3) where {T1<:Function,T2<:Function,T3<:Params{Zygote.Buffer{Any, Vector{Any}}}}
    println("training with $(vmc_solution.N) sample points and $(vmc_solution.epoch) iterations...")
    for _ in 1:vmc_solution.epoch
        ğ«=metropolis_hastings(
            vmc_solution.Ïµ,
            vmc_problem.Î©[1],vmc_problem.Î©[2],
            vmc_solution.N,
            x->sum(vmc_solution.Ïˆ(x).^2),
            distribution=vmc_solution.distribution,
            spatial_dimension=Int(vmc_problem.spatial_dimension),
            type=Float32,
            particle_number=vmc_problem.particle_number
            )

        Î”E=Î”Ïµ(vmc_solution.N,ğ«,vmc_solution.Ïˆ,HÌ‚,Îµâ‚€,Î˜) ## custom gradient 
        Flux.update!(opt,Î˜,Î”E)
    end 
    println("trained!")
end 