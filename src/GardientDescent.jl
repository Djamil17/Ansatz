"""

""" 

using ForwardDiff

mag(ùêØ::Vector)=sqrt(sum(ùêØ.^2))

‚àá=ForwardDiff.derivative
‚àá=ForwardDiff.gradient

##TODO : multidimensional gradient descent (done)
##TOD): stochastic gradient descent 
##TOD: adaptive gradient descents  

function gradient_descent(objective::Function, Œ∏::Float64, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Float64

    """

    gradient_descent(objective::Function, Œ∏::Float64, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Float64

    Performs gradient descent in n-d , for n variables, minimizes objective f with n arguments, given a guess vector Œ∏ and a learning rate Œ∑.
    Exits once the absolute value of the step Œ∑*||‚àá(f)|| < tolerance. 

    Examples
    ‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°

    julia> f(x)=x^2+2.324245x
    julia> gradient_descent(f,-1,.00001,1000000,1e-6)
    
    -1.1621220000061123


    """

    i=0
    x·µ¢=Œ∏
    while i<iterations
        loss=‚àá(objective,x·µ¢)
        x·µ¢‚Çä‚ÇÅ= x·µ¢-Œ∑* loss
        if abs(loss)<tolerance
            break
        end 
        x·µ¢=x·µ¢‚Çä‚ÇÅ
        i+=1
    end 

    return x·µ¢

end 


function gradient_descent(objective::Function, Œ∏::Array{Float64,1}, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Vector

    """

    gradient_descent(objective::Function, Œ∏::Float64, Œ∑::Float64 ,iterations::Int64,tolerance::Float64)::Float64

    Performs gradient descent in n-d , for n variables, minimizes objective f with n arguments, given a guess vector Œ∏ and a learning rate Œ∑.
    Exits once the absolute value of the step Œ∑*||‚àá(f)|| < tolerance. 

    Examples
    ‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°

    julia> f(x::Vector)=x[1]^2+2.324245x[2]
    julia> gradient_descent(f,[-1.0,-2.0],.00001,1000000,1e-6)
    
    2-element Array{Float64,1}:
    -2.0607414274390493e-9
    -25.242449999476282
        

    """

    i=0
    w·µ¢=Œ∏
    
    while i<iterations
        loss=‚àá(objective,w·µ¢)
        w·µ¢‚Çä‚ÇÅ= w·µ¢-Œ∑* loss
        if mag(loss)<tolerance
            break
        end 
        w·µ¢=w·µ¢‚Çä‚ÇÅ
        i+=1
    end 

    return w·µ¢

end 


z(ùê´::Vector)=-exp(-((ùê´[1])^2+(ùê´[2])^2))

gradient_descent(z,[-0.0,-2.0],.0001,1000000,1e-5 )